{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.dlc_practical_prologue as prologue\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shapetorch.Size([1000, 2, 14, 14])\n",
      "Train target shapetorch.Size([1000])\n",
      "Train classes shapetorch.Size([1000, 2])\n",
      "---\n",
      "Test input shape torch.Size([1000, 2, 14, 14])\n",
      "Test target shape torch.Size([1000])\n",
      "Test classes shape torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.div(255))]\n",
    ")\n",
    "\n",
    "train_input, train_target, train_classes,\\\n",
    "    test_input, test_target, test_classes = prologue.generate_pair_sets(\n",
    "        1000, transform=transform)\n",
    "    \n",
    "# train_input = transform(train_input)\n",
    "# test_input = transform(test_input)\n",
    "\n",
    "trainset = TensorDataset(train_input, train_target, train_classes)\n",
    "testset = TensorDataset(test_input, test_target, test_classes)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers= 1)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=BATCH_SIZE, shuffle=False, num_workers= 1)\n",
    "\n",
    "# target: 1 if digit 1 <= digit 2, \n",
    "#         0 otherwise\n",
    "\n",
    "# Plotting the first 5 pairs of images\n",
    "for i in range(1):\n",
    "    for j in range(2):\n",
    "        plt.subplot(1, 2, j+1)\n",
    "        plt.imshow(train_input[i, j].view(14, 14), cmap='gray')\n",
    "        # plot the target and its class in the title\n",
    "        plt.suptitle(f\"Target: {train_target[i]}\")\n",
    "        plt.title(f' Class: {train_classes[i, j]}')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# print the shapes of the tensors\n",
    "print(f\"Train input shape{train_input.shape}\")\n",
    "print(f\"Train target shape{train_target.shape}\")\n",
    "print(f\"Train classes shape{train_classes.shape}\")\n",
    "print('---')\n",
    "print(f\"Test input shape {test_input.shape}\")\n",
    "print(f\"Test target shape {test_target.shape}\")\n",
    "print(f\"Test classes shape {test_classes.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine data format and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 14])\n",
      "torch.float32\n",
      "tensor(0.9922)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(train_input[0,1].shape)\n",
    "print(train_input[0,1].dtype)\n",
    "print(train_input[0,1].max())\n",
    "print(train_input[0,1].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5530)\n",
      "tensor(0.5600)\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(train_target.float()))\n",
    "print(torch.mean(test_target.float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 56.00%\n"
     ]
    }
   ],
   "source": [
    "baseline_target_pred = torch.ones_like(train_target)\n",
    "\n",
    "# compute accuracy of the baseline and the test target\n",
    "acc = 100 * (baseline_target_pred == test_target).sum().item() / test_target.shape[0]\n",
    "print(f\"Baseline accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO now:\n",
    "\n",
    "1. figure out parameters for cnn layers\n",
    "\n",
    "    a. Figure out input (w/ 2 images)\n",
    "    \n",
    "    b. Debug conv and pool parameters\n",
    "\n",
    "2. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      \n",
    "      self.conv1 = nn.Conv2d(1, 16, 3)\n",
    "      self.pool1 = nn.MaxPool2d(2)\n",
    "      self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "      self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "      \n",
    "      self.fc1 = nn.Linear(64*2*2*2, 80)\n",
    "      self.fc1_aux = nn.Linear(64*2*2, 80)\n",
    "      self.fc2 = nn.Linear(80, 40)\n",
    "      \n",
    "      self.fc3 = nn.Linear(40, 1) # for target prediction\n",
    "      self.fc4 = nn.Linear(40,10) # for auxilary loss\n",
    "      \n",
    "      \n",
    "      \n",
    "   def forward(self, data):\n",
    "      x = torch.narrow(data,dim=1,start=0,length=1)\n",
    "      y = torch.narrow(data,dim=1,start=1,length=1)\n",
    "      \n",
    "      \n",
    "      x = self.pool1(F.relu(self.conv1(x)))\n",
    "      x = F.relu(self.conv2(x))\n",
    "      x = F.relu(self.conv3(x) )\n",
    "      x = x.view(x.size(0), -1)\n",
    "\n",
    "      \n",
    "      y = self.pool1(F.relu(self.conv1(y)))\n",
    "      y = F.relu(self.conv2(y))\n",
    "      y = F.relu(self.conv3(y))\n",
    "      y = y.view(y.size(0), -1)\n",
    "      \n",
    "      z = torch.cat((x,y), 1)\n",
    "      \n",
    "      z = F.relu(self.fc1(z))\n",
    "      z = F.relu(self.fc2(z))\n",
    "      res = torch.sigmoid(self.fc3(z))\n",
    "      \n",
    "      # embedded_images = torch.softmax(self.fc4(z), dim = 1)\n",
    "      # return embedded_images,res\n",
    "\n",
    "      emb_x = torch.softmax(F.relu(self.fc4(z)), dim = 1)\n",
    "      emb_y = torch.softmax(F.relu(self.fc4(z)), dim = 1)\n",
    "      return emb_x, emb_y, res\n",
    "      \n",
    "      \n",
    "   \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=80, bias=True)\n",
       "  (fc1_aux): Linear(in_features=256, out_features=80, bias=True)\n",
       "  (fc2): Linear(in_features=80, out_features=40, bias=True)\n",
       "  (fc3): Linear(in_features=40, out_features=1, bias=True)\n",
       "  (fc4): Linear(in_features=40, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "aux_criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.5)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training 2.128\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(25):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        ## -- ! code required  \n",
    "        \n",
    "        inputs, labels,classes = data\n",
    "\n",
    "        inputs, labels, classes = inputs.to(device), labels.to(device), classes.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output_x,output_y,output_z = net(inputs)\n",
    "                \n",
    "         # TODO : COMPUTE LOSS\n",
    "        loss_x = aux_criterion(output_x, classes[:,0])\n",
    "        loss_y = aux_criterion(output_y, classes[:,1])\n",
    "        loss_z = criterion(output_z, labels.reshape(-1,1).type_as(output_z[:,:10]))\n",
    "        loss = loss_x + loss_y + loss_z\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 0:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % \n",
    "                (epoch + 1, i + 1, running_loss / (1000/trainloader.batch_size)), end='\\r')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1000 test images: 86.5 %\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy(net, testloader):\n",
    "\n",
    "    acc = 0\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        \n",
    "        inputs, labels, classes = data\n",
    "\n",
    "        # Concatenate both images side-by-side\n",
    "        # left, right = inputs[:, 0], inputs[:, 1]\n",
    "        # inputs = torch.cat((left, right), 2)\n",
    "        # inputs = torch.unsqueeze(inputs, 1)\n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        inputs, labels, classes = inputs.to(device), labels.to(device), classes.to(device)\n",
    "    \n",
    "        _,_,output_z = net(inputs)\n",
    "        equality = (labels.reshape(-1, 1) == output_z.round().to(torch.int))\n",
    "        acc += equality.to(torch.float).mean()\n",
    "\n",
    "    return acc.item() / (i+1)\n",
    "\n",
    "acc = test_accuracy(net, testloader)\n",
    "print(f'Accuracy of the network on the {(i+1)*testloader.batch_size} test images: {100*acc:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 16, 12, 12]          160\n",
      "├─MaxPool2d: 1-2                         [-1, 16, 6, 6]            --\n",
      "├─Conv2d: 1-3                            [-1, 32, 4, 4]            4,640\n",
      "├─Conv2d: 1-4                            [-1, 64, 2, 2]            18,496\n",
      "├─Conv2d: 1-5                            [-1, 16, 12, 12]          (recursive)\n",
      "├─MaxPool2d: 1-6                         [-1, 16, 6, 6]            --\n",
      "├─Conv2d: 1-7                            [-1, 32, 4, 4]            (recursive)\n",
      "├─Conv2d: 1-8                            [-1, 64, 2, 2]            (recursive)\n",
      "├─Linear: 1-9                            [-1, 80]                  41,040\n",
      "├─Linear: 1-10                           [-1, 40]                  3,240\n",
      "├─Linear: 1-11                           [-1, 1]                   41\n",
      "├─Linear: 1-12                           [-1, 10]                  410\n",
      "├─Linear: 1-13                           [-1, 10]                  (recursive)\n",
      "==========================================================================================\n",
      "Total params: 68,027\n",
      "Trainable params: 68,027\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.38\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.26\n",
      "Estimated Total Size (MB): 0.29\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Conv2d: 1-1                            [-1, 16, 12, 12]          160\n",
       "├─MaxPool2d: 1-2                         [-1, 16, 6, 6]            --\n",
       "├─Conv2d: 1-3                            [-1, 32, 4, 4]            4,640\n",
       "├─Conv2d: 1-4                            [-1, 64, 2, 2]            18,496\n",
       "├─Conv2d: 1-5                            [-1, 16, 12, 12]          (recursive)\n",
       "├─MaxPool2d: 1-6                         [-1, 16, 6, 6]            --\n",
       "├─Conv2d: 1-7                            [-1, 32, 4, 4]            (recursive)\n",
       "├─Conv2d: 1-8                            [-1, 64, 2, 2]            (recursive)\n",
       "├─Linear: 1-9                            [-1, 80]                  41,040\n",
       "├─Linear: 1-10                           [-1, 40]                  3,240\n",
       "├─Linear: 1-11                           [-1, 1]                   41\n",
       "├─Linear: 1-12                           [-1, 10]                  410\n",
       "├─Linear: 1-13                           [-1, 10]                  (recursive)\n",
       "==========================================================================================\n",
       "Total params: 68,027\n",
       "Trainable params: 68,027\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.38\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.02\n",
       "Params size (MB): 0.26\n",
       "Estimated Total Size (MB): 0.29\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model_view = Net()\n",
    "summary(model_view,(2,14,14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model parameters\n",
    "torch.save(net.state_dict(), './Models/cnn_aux_loss.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=80, bias=True)\n",
       "  (fc1_aux): Linear(in_features=256, out_features=80, bias=True)\n",
       "  (fc2): Linear(in_features=80, out_features=40, bias=True)\n",
       "  (fc3): Linear(in_features=40, out_features=1, bias=True)\n",
       "  (fc4): Linear(in_features=40, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model parameters\n",
    "model1 = Net()\n",
    "model1.load_state_dict(torch.load('./Models/cnn_aux_loss.pt'))\n",
    "model1.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1df14ab64c8b80a00633eb588e502182d4703a9c23e6ea3c6fedede8ab1fffa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
