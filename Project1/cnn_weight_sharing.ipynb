{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.dlc_practical_prologue as prologue\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shapetorch.Size([1000, 2, 14, 14])\n",
      "Train target shapetorch.Size([1000])\n",
      "Train classes shapetorch.Size([1000, 2])\n",
      "---\n",
      "Test input shape torch.Size([1000, 2, 14, 14])\n",
      "Test target shape torch.Size([1000])\n",
      "Test classes shape torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.div(255))]\n",
    ")\n",
    "\n",
    "train_input, train_target, train_classes,\\\n",
    "    test_input, test_target, test_classes = prologue.generate_pair_sets(\n",
    "        1000, transform=transform)\n",
    "    \n",
    "# train_input = transform(train_input)\n",
    "# test_input = transform(test_input)\n",
    "\n",
    "trainset = TensorDataset(train_input, train_target)\n",
    "testset = TensorDataset(test_input, test_target)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers= 1)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=BATCH_SIZE, shuffle=False, num_workers= 1)\n",
    "\n",
    "# target: 1 if digit 1 <= digit 2, \n",
    "#         0 otherwise\n",
    "\n",
    "# Plotting the first 5 pairs of images\n",
    "for i in range(1):\n",
    "    for j in range(2):\n",
    "        plt.subplot(1, 2, j+1)\n",
    "        plt.imshow(train_input[i, j].view(14, 14), cmap='gray')\n",
    "        # plot the target and its class in the title\n",
    "        plt.suptitle(f\"Target: {train_target[i]}\")\n",
    "        plt.title(f' Class: {train_classes[i, j]}')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# print the shapes of the tensors\n",
    "print(f\"Train input shape{train_input.shape}\")\n",
    "print(f\"Train target shape{train_target.shape}\")\n",
    "print(f\"Train classes shape{train_classes.shape}\")\n",
    "print('---')\n",
    "print(f\"Test input shape {test_input.shape}\")\n",
    "print(f\"Test target shape {test_target.shape}\")\n",
    "print(f\"Test classes shape {test_classes.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine data format and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 14])\n",
      "torch.float32\n",
      "tensor(0.9931)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(train_input[0,1].shape)\n",
    "print(train_input[0,1].dtype)\n",
    "print(train_input[0,1].max())\n",
    "print(train_input[0,1].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5510)\n",
      "tensor(0.5260)\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(train_target.float()))\n",
    "print(torch.mean(test_target.float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 52.60%\n"
     ]
    }
   ],
   "source": [
    "baseline_target_pred = torch.ones_like(train_target)\n",
    "\n",
    "# compute accuracy of the baseline and the test target\n",
    "acc = 100 * (baseline_target_pred == test_target).sum().item() / test_target.shape[0]\n",
    "print(f\"Baseline accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO now:\n",
    "\n",
    "1. figure out parameters for cnn layers\n",
    "\n",
    "    a. Figure out input (w/ 2 images)\n",
    "    \n",
    "    b. Debug conv and pool parameters\n",
    "\n",
    "2. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 16, (3, 3))\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, (3,3))\n",
    "        self.conv3 = nn.Conv2d(32, 64, (3,3))\n",
    "        self.fc1 = nn.Linear(64*2*2, 200)\n",
    "        self.fc2 = nn.Linear(200, 1)\n",
    "        # self.fc3 = nn.Linear(32, 10)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = self.conv1(x)\n",
    "    #     x = F.relu(x)\n",
    "    #     x = self.pool(x)\n",
    "    #     x = self.conv2(x)\n",
    "    #     x = F.relu(x)\n",
    "    #     x = self.conv3(x)\n",
    "    #     x = F.relu(x)\n",
    "        \n",
    "    #     x = x.view(x.size(0), -1)   # flatten features\n",
    "        \n",
    "        \n",
    "    #     x = self.fc1(x)\n",
    "    #     x = self.fc2(F.relu(x))\n",
    "    #     x = F.softmax(x, dim=0) # return a probability\n",
    "        \n",
    "    #     return x\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # extract images from the pair\n",
    "        x1 = torch.narrow(x,dim=1,start=0,length=1)\n",
    "        x2 = torch.narrow(x,dim=1,start=1,length=1)\n",
    "        \n",
    "        \n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.pool(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "        x1 = x1.view(x1.size(0), -1)   # flatten features\n",
    "        \n",
    "        x2 = self.conv1(x2)\n",
    "        x2 = self.pool(x2)\n",
    "        x2 = self.conv2(x2)\n",
    "        x2 = x2.view(x2.size(0), -1)   # flatten features\n",
    "        \n",
    "        z = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = self.fc2(z)\n",
    "        z = F.softmax(z, dim=0) # return a probability\n",
    "        # x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "# net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      \n",
    "      self.conv1 = nn.Conv2d(1, 16, 3)\n",
    "      self.pool1 = nn.MaxPool2d(2)\n",
    "      self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "      self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "      \n",
    "      \n",
    "      self.fc1 = nn.Linear(64*2*2*2, 80)\n",
    "      self.fc2 = nn.Linear(80, 40)\n",
    "      self.fc3 = nn.Linear(40, 1)\n",
    "      \n",
    "   def forward(self, data):\n",
    "      x = torch.narrow(data,dim=1,start=0,length=1)\n",
    "      y = torch.narrow(data,dim=1,start=1,length=1)\n",
    "      \n",
    "      \n",
    "      x = self.pool1(F.relu(self.conv1(x)))\n",
    "      x = F.relu(self.conv2(x))\n",
    "      x = F.relu(self.conv3(x) )\n",
    "      x = x.view(x.size(0), -1)\n",
    "      \n",
    "      \n",
    "      y = self.pool1(F.relu(self.conv1(y)))\n",
    "      y = F.relu(self.conv2(y))\n",
    "      y = F.relu(self.conv3(y) )\n",
    "      y = y.view(y.size(0), -1)\n",
    "      \n",
    "      \n",
    "      z = torch.cat((x,y), 1)\n",
    "      \n",
    "      z = F.relu(self.fc1(z))\n",
    "      z = F.relu(self.fc2(z))\n",
    "      \n",
    "      z = torch.sigmoid(self.fc3(z))\n",
    "      return z\n",
    "   \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=80, bias=True)\n",
       "  (fc2): Linear(in_features=80, out_features=40, bias=True)\n",
       "  (fc3): Linear(in_features=40, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.5)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training 0.078\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(25):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        ## -- ! code required  \n",
    "        \n",
    "        \n",
    "        inputs, labels = data\n",
    "        # Concatenate both images side-by-side\n",
    "        # left, right = inputs[:, 0], inputs[:, 1]\n",
    "        # inputs = torch.cat((left, right), 2)\n",
    "        # inputs = torch.unsqueeze(inputs, 1)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        outputs = net(inputs)\n",
    "        # print(labels.reshape(-1, 1).type_as(outputs))\n",
    "        \n",
    "        loss = criterion(outputs, labels.reshape(-1, 1).type_as(outputs))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 0:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % \n",
    "                (epoch + 1, i + 1, running_loss / (1000/trainloader.batch_size)), end='\\r')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1000 test images: 85.7 %\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy(net, testloader):\n",
    "\n",
    "    acc = 0\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "\n",
    "        # Concatenate both images side-by-side\n",
    "        # left, right = inputs[:, 0], inputs[:, 1]\n",
    "        # inputs = torch.cat((left, right), 2)\n",
    "        # inputs = torch.unsqueeze(inputs, 1)\n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "        outputs = net(inputs)\n",
    "        equality = (labels.reshape(-1, 1) == outputs.round().to(torch.int))\n",
    "        acc += equality.to(torch.float).mean()\n",
    "\n",
    "    return acc.item() / (i+1)\n",
    "\n",
    "acc = test_accuracy(net, testloader)\n",
    "print(f'Accuracy of the network on the {(i+1)*testloader.batch_size} test images: {100*acc:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            160\n",
      "├─MaxPool2d: 1-2                         --\n",
      "├─Conv2d: 1-3                            4,640\n",
      "├─Conv2d: 1-4                            18,496\n",
      "├─Linear: 1-5                            41,040\n",
      "├─Linear: 1-6                            3,240\n",
      "├─Linear: 1-7                            41\n",
      "=================================================================\n",
      "Total params: 67,617\n",
      "Trainable params: 67,617\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Conv2d: 1-1                            160\n",
       "├─MaxPool2d: 1-2                         --\n",
       "├─Conv2d: 1-3                            4,640\n",
       "├─Conv2d: 1-4                            18,496\n",
       "├─Linear: 1-5                            41,040\n",
       "├─Linear: 1-6                            3,240\n",
       "├─Linear: 1-7                            41\n",
       "=================================================================\n",
       "Total params: 67,617\n",
       "Trainable params: 67,617\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model_view = Net()\n",
    "summary(model_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model parameters\n",
    "torch.save(net.state_dict(), './Models/cnn_ws.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=80, bias=True)\n",
       "  (fc2): Linear(in_features=80, out_features=40, bias=True)\n",
       "  (fc3): Linear(in_features=40, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model parameters\n",
    "model1 = Net()\n",
    "model1.load_state_dict(torch.load('./Models/cnn_ws.pt'))\n",
    "model1.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1df14ab64c8b80a00633eb588e502182d4703a9c23e6ea3c6fedede8ab1fffa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
